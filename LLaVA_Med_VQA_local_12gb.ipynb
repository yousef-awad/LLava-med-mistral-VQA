{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLaVA-Med for Binary Medical VQA on Chest X-rays\n",
    "\n",
    "**Model B: Vision-Language Model Approach**\n",
    "\n",
    "This notebook implements LLaVA-Med (Large Language and Vision Assistant for BioMedicine) for binary Visual Question Answering on the VQA-RAD dataset, focusing on Chest X-rays with Yes/No questions.\n",
    "\n",
    "We use Parameter-Efficient Fine-Tuning (PEFT) with LoRA to adapt the model to our specific task.\n",
    "\n",
    "**Requirements:** Local GPU with ~12 GB VRAM (4-bit quantization)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local setup (no Colab / no git clone)\n",
    "# Assumes a local LLaVA checkout at ./LLaVA\n",
    "\n",
    "import importlib.util\n",
    "\n",
    "required = [\n",
    "    \"torch\",\n",
    "    \"transformers\",\n",
    "    \"bitsandbytes\",\n",
    "    \"peft\",\n",
    "    \"sklearn\",\n",
    "    \"tqdm\",\n",
    "    \"seaborn\",\n",
    "    \"PIL\",\n",
    "]\n",
    "missing = [pkg for pkg in required if importlib.util.find_spec(pkg) is None]\n",
    "if missing:\n",
    "    print(\"Missing packages:\", missing)\n",
    "    print(\"Install locally before continuing, e.g.:\")\n",
    "    print(\"  pip install -r ./LLaVA/requirements.txt\")\n",
    "    print(\"  pip install bitsandbytes>=0.41.0 peft>=0.7.0 scikit-learn tqdm seaborn\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Hugging Face token for faster downloads (set in your environment)\n",
    "hf_token = os.environ.get(\"HUGGINGFACE_HUB_TOKEN\") or os.environ.get(\"HF_TOKEN\")\n",
    "if hf_token:\n",
    "    os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = hf_token\n",
    "    os.environ[\"HF_TOKEN\"] = hf_token\n",
    "    print(\"HF token detected for this session.\")\n",
    "else:\n",
    "    print(\"No HF token found. Set HUGGINGFACE_HUB_TOKEN or HF_TOKEN for faster downloads.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT_DIR = Path.cwd()\n",
    "LLAVA_DIR = ROOT_DIR / \"LLaVA\"\n",
    "if not LLAVA_DIR.exists():\n",
    "    raise FileNotFoundError(f\"LLaVA repo not found at: {LLAVA_DIR}\")\n",
    "\n",
    "sys.path.insert(0, str(LLAVA_DIR))\n",
    "\n",
    "import json\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Check GPU + VRAM (target: ~12 GB)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {vram_gb:.1f} GB\")\n",
    "    if vram_gb < 11.5:\n",
    "        print(\"Warning: GPU has < 12 GB VRAM. You may need to reduce batch size or max tokens.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility - SAME SEED as CNN baseline\n",
    "SEED = 777\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Loading and Preprocessing\n",
    "\n",
    "Using the **exact same filtering and splitting strategy** as the CNN baseline to ensure fair comparison.\n",
    "\n",
    "The VQA_RAD dataset should be in the local `./VQA_RAD` folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Dataset paths - VQA_RAD in the project folder\n",
    "ROOT_DIR = Path.cwd()\n",
    "DATA_DIR = ROOT_DIR / \"VQA_RAD\"\n",
    "IMAGE_DIR = DATA_DIR / \"VQA_RAD Image Folder\"\n",
    "JSON_PATH = DATA_DIR / \"VQA_RAD Dataset Public.json\"\n",
    "\n",
    "# Verify dataset exists\n",
    "if not JSON_PATH.exists():\n",
    "    print(\"Dataset not found at expected path!\")\n",
    "    print(f\"Expected: {JSON_PATH}\")\n",
    "    print(\"Please place the VQA_RAD folder here:\")\n",
    "    print(f\"  {DATA_DIR}\")\n",
    "    print(\"  VQA_RAD/\")\n",
    "    print(\"    VQA_RAD Image Folder/\")\n",
    "    print(\"      synpic0001.jpg\")\n",
    "    print(\"      ...\")\n",
    "    print(\"    VQA_RAD Dataset Public.json\")\n",
    "else:\n",
    "    print(\"Dataset found!\")\n",
    "    print(f\"Images: {IMAGE_DIR}\")\n",
    "    print(f\"JSON: {JSON_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data\n",
    "with open(JSON_PATH, \"r\") as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "print(f\"Total raw samples: {len(raw_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_answer(ans):\n",
    "    \"\"\"Normalize answer to binary label (same as CNN baseline).\"\"\"\n",
    "    ans = str(ans).lower().strip()\n",
    "    if ans in [\"yes\", \"y\"]:\n",
    "        return 1\n",
    "    if ans in [\"no\", \"n\"]:\n",
    "        return 0\n",
    "    return None\n",
    "\n",
    "# Filter dataset: Chest X-rays only, Binary (Yes/No) questions only\n",
    "# EXACT SAME FILTERING AS CNN BASELINE\n",
    "samples = []\n",
    "\n",
    "for item in raw_data:\n",
    "    # Filter for Chest images only\n",
    "    if item.get(\"image_organ\", \"\").lower() != \"chest\":\n",
    "        continue\n",
    "    \n",
    "    # Filter for closed-ended (binary) questions only\n",
    "    if item.get(\"answer_type\", \"\").lower() != \"closed\":\n",
    "        continue\n",
    "    \n",
    "    # Normalize answer to binary\n",
    "    label = normalize_answer(item.get(\"answer\", \"\"))\n",
    "    if label is None:\n",
    "        continue\n",
    "    \n",
    "    image_name = item.get(\"image_name\")\n",
    "    if image_name is None:\n",
    "        continue\n",
    "    \n",
    "    samples.append({\n",
    "        \"image_path\": os.path.join(IMAGE_DIR, image_name),\n",
    "        \"image_id\": image_name,\n",
    "        \"question\": item[\"question\"],\n",
    "        \"label\": label,\n",
    "        \"answer_text\": \"yes\" if label == 1 else \"no\"\n",
    "    })\n",
    "\n",
    "print(f\"Filtered samples (Chest X-ray, Binary): {len(samples)}\")\n",
    "\n",
    "# Check class distribution\n",
    "yes_count = sum(1 for s in samples if s[\"label\"] == 1)\n",
    "no_count = len(samples) - yes_count\n",
    "print(f\"Class distribution - Yes: {yes_count} ({yes_count/len(samples)*100:.1f}%), No: {no_count} ({no_count/len(samples)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_level_split(samples, seed=SEED):\n",
    "    \"\"\"\n",
    "    Image-level splitting to prevent data leakage.\n",
    "    All questions for the same image go to the same split.\n",
    "    EXACT SAME SPLITTING AS CNN BASELINE.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Group samples by image\n",
    "    image_to_samples = defaultdict(list)\n",
    "    for s in samples:\n",
    "        image_to_samples[s[\"image_id\"]].append(s)\n",
    "    \n",
    "    # Shuffle image IDs\n",
    "    image_ids = list(image_to_samples.keys())\n",
    "    random.shuffle(image_ids)\n",
    "    \n",
    "    # Split: 80% train, 10% val, 10% test\n",
    "    n = len(image_ids)\n",
    "    train_ids = image_ids[:int(0.8 * n)]\n",
    "    val_ids = image_ids[int(0.8 * n):int(0.9 * n)]\n",
    "    test_ids = image_ids[int(0.9 * n):]\n",
    "    \n",
    "    def collect(ids):\n",
    "        out = []\n",
    "        for i in ids:\n",
    "            out.extend(image_to_samples[i])\n",
    "        return out\n",
    "    \n",
    "    return collect(train_ids), collect(val_ids), collect(test_ids)\n",
    "\n",
    "train_samples, val_samples, test_samples = image_level_split(samples)\n",
    "print(f\"Train: {len(train_samples)}, Val: {len(val_samples)}, Test: {len(test_samples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify split matches CNN baseline (should be 365, 49, 63)\n",
    "print(f\"\\nExpected from CNN baseline: Train=365, Val=49, Test=63\")\n",
    "print(f\"Actual: Train={len(train_samples)}, Val={len(val_samples)}, Test={len(test_samples)}\")\n",
    "\n",
    "# Inspect some samples\n",
    "print(\"\\nSample questions:\")\n",
    "for i, s in enumerate(train_samples[:3]):\n",
    "    print(f\"  {i+1}. Q: {s['question']}\")\n",
    "    print(f\"     A: {s['answer_text']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load LLaVA-Med Model\n",
    "\n",
    "Using the official LLaVA codebase with 4-bit quantization to fit in ~12 GB VRAM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import get_model_name_from_path, process_images, tokenizer_image_token\n",
    "from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
    "from llava.conversation import conv_templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "MODEL_PATH = os.environ.get(\"LLAVA_MED_MODEL_PATH\", \"microsoft/llava-med-v1.5-mistral-7b\")\n",
    "MAX_VRAM_GB = 12\n",
    "\n",
    "if Path(MODEL_PATH).exists():\n",
    "    print(f\"Using local model path: {MODEL_PATH}\")\n",
    "else:\n",
    "    print(f\"Using Hugging Face model ID: {MODEL_PATH}\")\n",
    "    print(\"Tip: set LLAVA_MED_MODEL_PATH to a local folder to avoid downloads.\")\n",
    "\n",
    "print(\"Loading LLaVA-Med model (this may take a few minutes)...\")\n",
    "\n",
    "max_memory = None\n",
    "if torch.cuda.is_available():\n",
    "    max_memory = {i: f\"{MAX_VRAM_GB}GB\" for i in range(torch.cuda.device_count())}\n",
    "\n",
    "# Use HF token from env if present (avoids guest rate limits)\n",
    "hf_token = os.environ.get(\"HUGGINGFACE_HUB_TOKEN\") or os.environ.get(\"HF_TOKEN\")\n",
    "\n",
    "# Newer transformers disallow load_in_4bit with quantization_config.\n",
    "# Use quantization_config only (see LLaVA issue #1638).\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "MODEL_LOAD_KWARGS = dict(\n",
    "    model_path=MODEL_PATH,\n",
    "    model_base=None,\n",
    "    model_name=get_model_name_from_path(MODEL_PATH),\n",
    "    load_4bit=False,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "if max_memory is not None:\n",
    "    MODEL_LOAD_KWARGS[\"max_memory\"] = max_memory\n",
    "if hf_token:\n",
    "    MODEL_LOAD_KWARGS[\"token\"] = hf_token\n",
    "\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(**MODEL_LOAD_KWARGS)\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Context length: {context_len}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check conversation template\n",
    "CONV_MODE = \"mistral_instruct\"  # LLaVA-Med v1.5 uses Mistral\n",
    "print(f\"Conversation mode: {CONV_MODE}\")\n",
    "\n",
    "# Test conversation template\n",
    "conv = conv_templates[CONV_MODE].copy()\n",
    "print(f\"System message: {conv.system}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Setup LoRA for Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "\n",
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA configuration targeting the language model layers\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                          # Rank of the update matrices\n",
    "    lora_alpha=32,                 # Scaling factor\n",
    "    lora_dropout=0.1,              # Dropout probability\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[               # Target attention and MLP layers\n",
    "        \"q_proj\",\n",
    "        \"k_proj\", \n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inference Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt for binary VQA\n",
    "SYSTEM_PROMPT = \"\"\"You are a medical assistant specialized in analyzing Chest X-rays. Answer the following question about this Chest X-ray with ONLY 'yes' or 'no'. Do not provide any explanation.\"\"\"\n",
    "\n",
    "def create_prompt(question):\n",
    "    \"\"\"Create the prompt for the model.\"\"\"\n",
    "    return f\"{SYSTEM_PROMPT}\\n\\nQuestion: {question}\\nAnswer:\"\n",
    "\n",
    "def prepare_inputs(image_path, question, tokenizer, image_processor, model):\n",
    "    \"\"\"\n",
    "    Prepare inputs for the model.\n",
    "    \"\"\"\n",
    "    # Load and process image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image_tensor = process_images([image], image_processor, model.config)\n",
    "    image_tensor = image_tensor.to(model.device, dtype=torch.float16)\n",
    "    \n",
    "    # Create conversation\n",
    "    conv = conv_templates[CONV_MODE].copy()\n",
    "    \n",
    "    # Add image token and question\n",
    "    prompt = create_prompt(question)\n",
    "    inp = DEFAULT_IMAGE_TOKEN + \"\\n\" + prompt\n",
    "    \n",
    "    conv.append_message(conv.roles[0], inp)\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "    \n",
    "    full_prompt = conv.get_prompt()\n",
    "    \n",
    "    # Tokenize\n",
    "    input_ids = tokenizer_image_token(\n",
    "        full_prompt, \n",
    "        tokenizer, \n",
    "        IMAGE_TOKEN_INDEX, \n",
    "        return_tensors=\"pt\"\n",
    "    ).unsqueeze(0).to(model.device)\n",
    "    \n",
    "    return input_ids, image_tensor\n",
    "\n",
    "def parse_yes_no(text):\n",
    "    \"\"\"\n",
    "    Parse model output to extract yes/no prediction.\n",
    "    Returns 1 for yes, 0 for no, -1 if unable to parse.\n",
    "    \"\"\"\n",
    "    text = text.lower().strip()\n",
    "    \n",
    "    # Direct match at start\n",
    "    if text.startswith(\"yes\"):\n",
    "        return 1\n",
    "    if text.startswith(\"no\"):\n",
    "        return 0\n",
    "    \n",
    "    # Check first word\n",
    "    first_word = text.split()[0] if text.split() else \"\"\n",
    "    first_word = first_word.rstrip(\".,!?\")\n",
    "    if first_word == \"yes\":\n",
    "        return 1\n",
    "    if first_word == \"no\":\n",
    "        return 0\n",
    "    \n",
    "    # Search in text\n",
    "    if \"yes\" in text and \"no\" not in text:\n",
    "        return 1\n",
    "    if \"no\" in text and \"yes\" not in text:\n",
    "        return 0\n",
    "    \n",
    "    return -1  # Unable to parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_answer(image_path, question, tokenizer, image_processor, model, max_new_tokens=10, return_debug=False):\n",
    "    \"\"\"\n",
    "    Generate an answer for a given image and question.\n",
    "    \"\"\"\n",
    "    input_ids, image_tensor = prepare_inputs(\n",
    "        image_path, question, tokenizer, image_processor, model\n",
    "    )\n",
    "\n",
    "    # Generate\n",
    "    with torch.inference_mode():\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            images=image_tensor,\n",
    "            do_sample=False,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            use_cache=True,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "\n",
    "    prompt_len = int(input_ids.shape[1])\n",
    "    output_len = int(output_ids.shape[1])\n",
    "    output_includes_prompt = output_len >= prompt_len\n",
    "\n",
    "    if output_includes_prompt:\n",
    "        decoded_new_raw = tokenizer.decode(\n",
    "            output_ids[0, prompt_len:],\n",
    "            skip_special_tokens=False\n",
    "        )\n",
    "        generated = tokenizer.decode(\n",
    "            output_ids[0, prompt_len:],\n",
    "            skip_special_tokens=True\n",
    "        ).strip()\n",
    "        new_tokens = output_len - prompt_len\n",
    "    else:\n",
    "        # Some LLaVA generate paths return only new tokens (no prompt).\n",
    "        decoded_new_raw = tokenizer.decode(\n",
    "            output_ids[0],\n",
    "            skip_special_tokens=False\n",
    "        )\n",
    "        generated = tokenizer.decode(\n",
    "            output_ids[0],\n",
    "            skip_special_tokens=True\n",
    "        ).strip()\n",
    "        new_tokens = output_len\n",
    "\n",
    "    if not return_debug:\n",
    "        return generated\n",
    "\n",
    "    return {\n",
    "        \"generated\": generated,\n",
    "        \"decoded_new_raw\": decoded_new_raw,\n",
    "        \"prompt_len\": prompt_len,\n",
    "        \"output_len\": output_len,\n",
    "        \"new_tokens\": new_tokens,\n",
    "        \"output_includes_prompt\": output_includes_prompt,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference on a sample\n",
    "test_sample = test_samples[0]\n",
    "print(f\"Testing inference...\")\n",
    "print(f\"Question: {test_sample['question']}\")\n",
    "print(f\"True answer: {test_sample['answer_text']}\")\n",
    "\n",
    "response = generate_answer(\n",
    "    test_sample[\"image_path\"],\n",
    "    test_sample[\"question\"],\n",
    "    tokenizer, image_processor, model\n",
    ")\n",
    "print(f\"Model response: {response}\")\n",
    "print(f\"Parsed: {'yes' if parse_yes_no(response) == 1 else 'no' if parse_yes_no(response) == 0 else 'unparseable'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import ImageEnhance\n",
    "\n",
    "AUGMENT_PROB = 0.5\n",
    "ROTATE_MAX_DEG = 5\n",
    "BRIGHTNESS_RANGE = (0.9, 1.1)\n",
    "\n",
    "def apply_conservative_augmentation(image):\n",
    "    \"\"\"Light augmentation for X-rays. No flips to preserve laterality.\"\"\"\n",
    "    if random.random() < AUGMENT_PROB:\n",
    "        angle = random.uniform(-ROTATE_MAX_DEG, ROTATE_MAX_DEG)\n",
    "        image = image.rotate(angle, resample=Image.BILINEAR, fillcolor=0)\n",
    "    if random.random() < AUGMENT_PROB:\n",
    "        factor = random.uniform(*BRIGHTNESS_RANGE)\n",
    "        image = ImageEnhance.Brightness(image).enhance(factor)\n",
    "    return image\n",
    "\n",
    "class VQARadTrainDataset(Dataset):\n",
    "    \"\"\"Dataset for training with LLaVA-Med.\"\"\"\n",
    "    \n",
    "    def __init__(self, samples, tokenizer, image_processor, model_config, max_length=512, augment=False):\n",
    "        self.samples = samples\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_processor = image_processor\n",
    "        self.model_config = model_config\n",
    "        self.max_length = max_length\n",
    "        self.augment = augment\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        \n",
    "        # Load and process image\n",
    "        image = Image.open(sample[\"image_path\"]).convert(\"RGB\")\n",
    "        if self.augment:\n",
    "            image = apply_conservative_augmentation(image)\n",
    "        image_tensor = process_images([image], self.image_processor, self.model_config)[0]\n",
    "        \n",
    "        # Create conversation with answer\n",
    "        conv = conv_templates[CONV_MODE].copy()\n",
    "        prompt = create_prompt(sample[\"question\"])\n",
    "        inp = DEFAULT_IMAGE_TOKEN + \"\\n\" + prompt\n",
    "        \n",
    "        conv.append_message(conv.roles[0], inp)\n",
    "        conv.append_message(conv.roles[1], sample[\"answer_text\"])\n",
    "        \n",
    "        full_text = conv.get_prompt()\n",
    "        \n",
    "        # Tokenize full conversation\n",
    "        input_ids = tokenizer_image_token(\n",
    "            full_text, \n",
    "            self.tokenizer, \n",
    "            IMAGE_TOKEN_INDEX, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Create labels (mask everything except the answer)\n",
    "        labels = input_ids.clone()\n",
    "        \n",
    "        # Find where the answer starts and mask everything before\n",
    "        # The answer is after the assistant's turn marker\n",
    "        conv_no_answer = conv_templates[CONV_MODE].copy()\n",
    "        conv_no_answer.append_message(conv_no_answer.roles[0], inp)\n",
    "        conv_no_answer.append_message(conv_no_answer.roles[1], None)\n",
    "        prompt_only = conv_no_answer.get_prompt()\n",
    "        \n",
    "        prompt_ids = tokenizer_image_token(\n",
    "            prompt_only, \n",
    "            self.tokenizer, \n",
    "            IMAGE_TOKEN_INDEX, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        prompt_len = prompt_ids.shape[0]\n",
    "        \n",
    "        # Mask prompt in labels\n",
    "        labels[:prompt_len] = -100\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"labels\": labels,\n",
    "            \"images\": image_tensor\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"Collate function for DataLoader.\"\"\"\n",
    "    # Pad input_ids and labels\n",
    "    max_len = max(item[\"input_ids\"].shape[0] for item in batch)\n",
    "    \n",
    "    input_ids_list = []\n",
    "    labels_list = []\n",
    "    images_list = []\n",
    "    attention_mask_list = []\n",
    "    \n",
    "    for item in batch:\n",
    "        seq_len = item[\"input_ids\"].shape[0]\n",
    "        pad_len = max_len - seq_len\n",
    "        \n",
    "        # Pad input_ids with pad_token_id\n",
    "        padded_input_ids = torch.cat([\n",
    "            item[\"input_ids\"],\n",
    "            torch.full((pad_len,), tokenizer.pad_token_id, dtype=torch.long)\n",
    "        ])\n",
    "        input_ids_list.append(padded_input_ids)\n",
    "        \n",
    "        # Pad labels with -100\n",
    "        padded_labels = torch.cat([\n",
    "            item[\"labels\"],\n",
    "            torch.full((pad_len,), -100, dtype=torch.long)\n",
    "        ])\n",
    "        labels_list.append(padded_labels)\n",
    "        \n",
    "        # Attention mask\n",
    "        attention_mask = torch.cat([\n",
    "            torch.ones(seq_len, dtype=torch.long),\n",
    "            torch.zeros(pad_len, dtype=torch.long)\n",
    "        ])\n",
    "        attention_mask_list.append(attention_mask)\n",
    "        \n",
    "        images_list.append(item[\"images\"])\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": torch.stack(input_ids_list),\n",
    "        \"labels\": torch.stack(labels_list),\n",
    "        \"attention_mask\": torch.stack(attention_mask_list),\n",
    "        \"images\": torch.stack(images_list)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = VQARadTrainDataset(\n",
    "    train_samples, tokenizer, image_processor, model.config, augment=True\n",
    ")\n",
    "\n",
    "val_dataset = VQARadTrainDataset(\n",
    "    val_samples, tokenizer, image_processor, model.config, augment=False\n",
    ")\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset)} samples\")\n",
    "print(f\"Val dataset: {len(val_dataset)} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "BATCH_SIZE = 1\n",
    "GRADIENT_ACCUMULATION = 16  # Effective batch size = 16\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 10\n",
    "WARMUP_RATIO = 0.1\n",
    "WEIGHT_DECAY = 0.01\n",
    "PATIENCE = 3  # For early stopping\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "# Scheduler\n",
    "total_steps = len(train_loader) * NUM_EPOCHS // GRADIENT_ACCUMULATION\n",
    "warmup_steps = int(total_steps * WARMUP_RATIO)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, scheduler, gradient_accumulation):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    pbar = tqdm(loader, desc=\"Training\")\n",
    "    for step, batch in enumerate(pbar):\n",
    "        input_ids = batch[\"input_ids\"].to(model.device)\n",
    "        labels = batch[\"labels\"].to(model.device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(model.device)\n",
    "        images = batch[\"images\"].to(model.device, dtype=torch.float16)\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            images=images\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss / gradient_accumulation\n",
    "        loss.backward()\n",
    "        \n",
    "        total_loss += loss.item() * gradient_accumulation\n",
    "        \n",
    "        if (step + 1) % gradient_accumulation == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        pbar.set_postfix({\"loss\": f\"{total_loss / (step + 1):.4f}\"})\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_loss(model, loader):\n",
    "    \"\"\"Evaluate loss on validation set.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(loader, desc=\"Evaluating\"):\n",
    "        input_ids = batch[\"input_ids\"].to(model.device)\n",
    "        labels = batch[\"labels\"].to(model.device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(model.device)\n",
    "        images = batch[\"images\"].to(model.device, dtype=torch.float16)\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            images=images\n",
    "        )\n",
    "        \n",
    "        total_loss += outputs.loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with early stopping\n",
    "best_val_loss = float(\"inf\")\n",
    "patience_counter = 0\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, scheduler, GRADIENT_ACCUMULATION)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss = evaluate_loss(model, val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # Check for improvement\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        # Save best model\n",
    "        model.save_pretrained(\"./llava_med_best\")\n",
    "        print(\"  -> New best model saved!\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"  -> No improvement (patience: {patience_counter}/{PATIENCE})\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= PATIENCE:\n",
    "        print(f\"\\nEarly stopping triggered at epoch {epoch + 1}\")\n",
    "        break\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(train_losses)+1), train_losses, label=\"Train Loss\", marker=\"o\")\n",
    "plt.plot(range(1, len(val_losses)+1), val_losses, label=\"Val Loss\", marker=\"s\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"LLaVA-Med Training Curves\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(\"llava_med_training_curves.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Load Best Model and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "from peft import PeftModel\n",
    "\n",
    "# Reload base model (same 4-bit quantization settings)\n",
    "tokenizer, base_model, image_processor, context_len = load_pretrained_model(**MODEL_LOAD_KWARGS)\n",
    "\n",
    "# Load LoRA weights\n",
    "model = PeftModel.from_pretrained(base_model, \"./llava_med_best\")\n",
    "model.eval()\n",
    "print(\"Best model loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_accuracy(samples, desc=\"Evaluating\", debug=False, debug_n=5):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a set of samples.\n",
    "    Returns predictions, labels, and raw outputs.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    raw_outputs = []\n",
    "    unparseable = 0\n",
    "\n",
    "    import inspect\n",
    "    supports_return_debug = \"return_debug\" in inspect.signature(generate_answer).parameters\n",
    "    if debug and not supports_return_debug:\n",
    "        print(\"[DEBUG] generate_answer does not support return_debug; re-run its definition cell to enable raw token logging.\")\n",
    "\n",
    "    for i, sample in enumerate(tqdm(samples, desc=desc)):\n",
    "        # Generate answer\n",
    "        if debug and i < debug_n and supports_return_debug:\n",
    "            response_debug = generate_answer(\n",
    "                sample[\"image_path\"],\n",
    "                sample[\"question\"],\n",
    "                tokenizer, image_processor, model,\n",
    "                return_debug=True\n",
    "            )\n",
    "            response = response_debug[\"generated\"]\n",
    "        else:\n",
    "            response_debug = None\n",
    "            response = generate_answer(\n",
    "                sample[\"image_path\"],\n",
    "                sample[\"question\"],\n",
    "                tokenizer, image_processor, model\n",
    "            )\n",
    "\n",
    "        raw_outputs.append(response)\n",
    "\n",
    "        # Parse prediction\n",
    "        pred = parse_yes_no(response)\n",
    "        if pred == -1:\n",
    "            unparseable += 1\n",
    "            pred = 0  # Default to 'no' for unparseable\n",
    "\n",
    "        predictions.append(pred)\n",
    "        labels.append(sample[\"label\"])\n",
    "\n",
    "        if debug and i < debug_n:\n",
    "            true_label = sample.get(\"answer_text\")\n",
    "            if true_label is None:\n",
    "                true_label = \"yes\" if sample[\"label\"] == 1 else \"no\"\n",
    "            if response_debug is not None:\n",
    "                raw = response_debug[\"decoded_new_raw\"]\n",
    "                raw_preview = raw[:200] + (\"...\" if len(raw) > 200 else \"\")\n",
    "                prompt_len = response_debug[\"prompt_len\"]\n",
    "                output_len = response_debug[\"output_len\"]\n",
    "                new_tokens = response_debug[\"new_tokens\"]\n",
    "                output_includes_prompt = response_debug[\"output_includes_prompt\"]\n",
    "            else:\n",
    "                raw_preview = \"(unavailable: return_debug not supported)\"\n",
    "                prompt_len = \"?\"\n",
    "                output_len = \"?\"\n",
    "                new_tokens = \"?\"\n",
    "                output_includes_prompt = \"?\"\n",
    "            print(\n",
    "                f\"[DEBUG {desc} #{i}] question={sample.get('question', '')!r} \"\n",
    "                f\"true={true_label} pred={pred} generated={response!r} \"\n",
    "                f\"prompt_len={prompt_len} \"\n",
    "                f\"output_len={output_len} \"\n",
    "                f\"new_tokens={new_tokens} \"\n",
    "                f\"output_includes_prompt={output_includes_prompt} \"\n",
    "                f\"raw_preview={raw_preview!r}\"\n",
    "            )\n",
    "\n",
    "    if unparseable > 0:\n",
    "        print(f\"Warning: {unparseable} outputs could not be parsed ({unparseable/len(samples)*100:.1f}%)\")\n",
    "\n",
    "    return predictions, labels, raw_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "print(\"Evaluating on validation set...\")\n",
    "DEBUG_EVAL = True\n",
    "DEBUG_EVAL_N = 5\n",
    "val_preds, val_labels, val_outputs = evaluate_accuracy(val_samples, \"Val\", debug=DEBUG_EVAL, debug_n=DEBUG_EVAL_N)\n",
    "\n",
    "val_acc = accuracy_score(val_labels, val_preds)\n",
    "val_f1 = f1_score(val_labels, val_preds, average=\"macro\")\n",
    "\n",
    "print(f\"\\nValidation Results:\")\n",
    "print(f\"  Accuracy: {val_acc:.4f}\")\n",
    "print(f\"  Macro-F1: {val_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "DEBUG_EVAL = True\n",
    "DEBUG_EVAL_N = 5\n",
    "test_preds, test_labels, test_outputs = evaluate_accuracy(test_samples, \"Test\", debug=DEBUG_EVAL, debug_n=DEBUG_EVAL_N)\n",
    "\n",
    "test_acc = accuracy_score(test_labels, test_preds)\n",
    "test_f1 = f1_score(test_labels, test_preds, average=\"macro\")\n",
    "\n",
    "print(f\"\\nTest Results:\")\n",
    "print(f\"  Accuracy: {test_acc:.4f}\")\n",
    "print(f\"  Macro-F1: {test_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report\n",
    "print(\"\\nClassification Report (Test Set):\")\n",
    "print(classification_report(test_labels, test_preds, target_names=[\"No\", \"Yes\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=[\"No\", \"Yes\"], yticklabels=[\"No\", \"Yes\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"LLaVA-Med Confusion Matrix (Test Set)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"llava_med_confusion_matrix.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze errors\n",
    "errors = []\n",
    "for i, (pred, label, output, sample) in enumerate(zip(test_preds, test_labels, test_outputs, test_samples)):\n",
    "    if pred != label:\n",
    "        errors.append({\n",
    "            \"question\": sample[\"question\"],\n",
    "            \"true_label\": \"yes\" if label == 1 else \"no\",\n",
    "            \"predicted\": \"yes\" if pred == 1 else \"no\",\n",
    "            \"raw_output\": output,\n",
    "            \"image_id\": sample[\"image_id\"]\n",
    "        })\n",
    "\n",
    "print(f\"Total errors: {len(errors)} / {len(test_samples)} ({len(errors)/len(test_samples)*100:.1f}%)\")\n",
    "print(\"\\nSample errors:\")\n",
    "for e in errors[:10]:\n",
    "    print(f\"  Q: {e['question']}\")\n",
    "    print(f\"  True: {e['true_label']}, Pred: {e['predicted']}, Raw: '{e['raw_output']}'\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze error patterns\n",
    "negation_keywords = [\"no \", \"not \", \"without\", \"absence\", \"negative\", \"isn't\", \"aren't\"]\n",
    "spatial_keywords = [\"left\", \"right\", \"upper\", \"lower\", \"middle\", \"bilateral\", \"base\", \"apex\"]\n",
    "\n",
    "if len(errors) > 0:\n",
    "    negation_errors = sum(1 for e in errors if any(kw in e[\"question\"].lower() for kw in negation_keywords))\n",
    "    spatial_errors = sum(1 for e in errors if any(kw in e[\"question\"].lower() for kw in spatial_keywords))\n",
    "    \n",
    "    print(f\"\\nError Analysis:\")\n",
    "    print(f\"  Errors involving negation: {negation_errors} ({negation_errors/len(errors)*100:.1f}% of errors)\")\n",
    "    print(f\"  Errors involving spatial terms: {spatial_errors} ({spatial_errors/len(errors)*100:.1f}% of errors)\")\n",
    "else:\n",
    "    print(\"No errors to analyze!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Comparison with CNN Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Baseline results (from teammate's notebook AA.ipynb)\n",
    "cnn_results = {\n",
    "    \"model\": \"ResNet50 + BiLSTM\",\n",
    "    \"test_acc\": 0.5556,\n",
    "    \"test_f1\": 0.5288\n",
    "}\n",
    "\n",
    "# LLaVA-Med results\n",
    "vlm_results = {\n",
    "    \"model\": \"LLaVA-Med (LoRA)\",\n",
    "    \"test_acc\": test_acc,\n",
    "    \"test_f1\": test_f1\n",
    "}\n",
    "\n",
    "print(\"=\"*65)\n",
    "print(\"COMPARISON: CNN Baseline vs. LLaVA-Med VLM\")\n",
    "print(\"=\"*65)\n",
    "print(f\"\\n{'Model':<25} {'Test Accuracy':<15} {'Test Macro-F1':<15}\")\n",
    "print(\"-\"*55)\n",
    "print(f\"{cnn_results['model']:<25} {cnn_results['test_acc']:<15.4f} {cnn_results['test_f1']:<15.4f}\")\n",
    "print(f\"{vlm_results['model']:<25} {vlm_results['test_acc']:<15.4f} {vlm_results['test_f1']:<15.4f}\")\n",
    "print(\"-\"*55)\n",
    "\n",
    "acc_diff = vlm_results['test_acc'] - cnn_results['test_acc']\n",
    "f1_diff = vlm_results['test_f1'] - cnn_results['test_f1']\n",
    "print(f\"{'Difference':<25} {acc_diff:+.4f}          {f1_diff:+.4f}\")\n",
    "print(\"=\"*65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of comparison\n",
    "models = [\"ResNet50+BiLSTM\\n(CNN Baseline)\", \"LLaVA-Med\\n(VLM + LoRA)\"]\n",
    "accuracy = [cnn_results[\"test_acc\"], vlm_results[\"test_acc\"]]\n",
    "f1_scores = [cnn_results[\"test_f1\"], vlm_results[\"test_f1\"]]\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars1 = ax.bar(x - width/2, accuracy, width, label=\"Accuracy\", color=\"steelblue\")\n",
    "bars2 = ax.bar(x + width/2, f1_scores, width, label=\"Macro-F1\", color=\"coral\")\n",
    "\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_title(\"Model Comparison: CNN Baseline vs. LLaVA-Med VLM\\nBinary VQA on Chest X-rays (VQA-RAD)\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models)\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax.annotate(f'{height:.3f}',\n",
    "                xy=(bar.get_x() + bar.get_width()/2, height),\n",
    "                xytext=(0, 3), textcoords=\"offset points\",\n",
    "                ha='center', va='bottom', fontsize=11)\n",
    "\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    ax.annotate(f'{height:.3f}',\n",
    "                xy=(bar.get_x() + bar.get_width()/2, height),\n",
    "                xytext=(0, 3), textcoords=\"offset points\",\n",
    "                ha='center', va='bottom', fontsize=11)\n",
    "\n",
    "plt.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, label='Random baseline')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"model_comparison.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Zero-Shot Evaluation (Baseline)\n",
    "\n",
    "Evaluate LLaVA-Med WITHOUT fine-tuning to measure the benefit of LoRA adaptation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model (without LoRA) for zero-shot evaluation\n",
    "print(\"Loading base LLaVA-Med for zero-shot evaluation...\")\n",
    "\n",
    "tokenizer_zs, model_zs, image_processor_zs, _ = load_pretrained_model(**MODEL_LOAD_KWARGS)\n",
    "model_zs.eval()\n",
    "\n",
    "print(\"Base model loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_zero_shot(samples, model_zs, tokenizer_zs, image_processor_zs, desc=\"Zero-shot\", debug=False, debug_n=5):\n",
    "    \"\"\"Evaluate zero-shot performance.\"\"\"\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    raw_outputs = []\n",
    "    unparseable = 0\n",
    "\n",
    "    import inspect\n",
    "    supports_return_debug = \"return_debug\" in inspect.signature(generate_answer).parameters\n",
    "    if debug and not supports_return_debug:\n",
    "        print(\"[DEBUG] generate_answer does not support return_debug; re-run its definition cell to enable raw token logging.\")\n",
    "\n",
    "    for i, sample in enumerate(tqdm(samples, desc=desc)):\n",
    "        if debug and i < debug_n and supports_return_debug:\n",
    "            response_debug = generate_answer(\n",
    "                sample[\"image_path\"],\n",
    "                sample[\"question\"],\n",
    "                tokenizer_zs, image_processor_zs, model_zs,\n",
    "                return_debug=True\n",
    "            )\n",
    "            response = response_debug[\"generated\"]\n",
    "        else:\n",
    "            response_debug = None\n",
    "            response = generate_answer(\n",
    "                sample[\"image_path\"],\n",
    "                sample[\"question\"],\n",
    "                tokenizer_zs, image_processor_zs, model_zs\n",
    "            )\n",
    "\n",
    "        raw_outputs.append(response)\n",
    "\n",
    "        pred = parse_yes_no(response)\n",
    "        if pred == -1:\n",
    "            unparseable += 1\n",
    "            pred = 0\n",
    "\n",
    "        predictions.append(pred)\n",
    "        labels.append(sample[\"label\"])\n",
    "\n",
    "        if debug and i < debug_n:\n",
    "            true_label = sample.get(\"answer_text\")\n",
    "            if true_label is None:\n",
    "                true_label = \"yes\" if sample[\"label\"] == 1 else \"no\"\n",
    "            if response_debug is not None:\n",
    "                raw = response_debug[\"decoded_new_raw\"]\n",
    "                raw_preview = raw[:200] + (\"...\" if len(raw) > 200 else \"\")\n",
    "                prompt_len = response_debug[\"prompt_len\"]\n",
    "                output_len = response_debug[\"output_len\"]\n",
    "                new_tokens = response_debug[\"new_tokens\"]\n",
    "                output_includes_prompt = response_debug[\"output_includes_prompt\"]\n",
    "            else:\n",
    "                raw_preview = \"(unavailable: return_debug not supported)\"\n",
    "                prompt_len = \"?\"\n",
    "                output_len = \"?\"\n",
    "                new_tokens = \"?\"\n",
    "                output_includes_prompt = \"?\"\n",
    "            print(\n",
    "                f\"[DEBUG {desc} #{i}] question={sample.get('question', '')!r} \"\n",
    "                f\"true={true_label} pred={pred} generated={response!r} \"\n",
    "                f\"prompt_len={prompt_len} \"\n",
    "                f\"output_len={output_len} \"\n",
    "                f\"new_tokens={new_tokens} \"\n",
    "                f\"output_includes_prompt={output_includes_prompt} \"\n",
    "                f\"raw_preview={raw_preview!r}\"\n",
    "            )\n",
    "\n",
    "    if unparseable > 0:\n",
    "        print(f\"Warning: {unparseable} outputs unparseable ({unparseable/len(samples)*100:.1f}%)\")\n",
    "\n",
    "    return predictions, labels, raw_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-shot evaluation on test set\n",
    "print(\"Zero-shot evaluation on test set...\")\n",
    "DEBUG_EVAL = True\n",
    "DEBUG_EVAL_N = 5\n",
    "zs_preds, zs_labels, zs_outputs = evaluate_zero_shot(\n",
    "    test_samples, model_zs, tokenizer_zs, image_processor_zs, \"Zero-shot Test\", debug=DEBUG_EVAL, debug_n=DEBUG_EVAL_N\n",
    ")\n",
    "\n",
    "zs_acc = accuracy_score(zs_labels, zs_preds)\n",
    "zs_f1 = f1_score(zs_labels, zs_preds, average=\"macro\")\n",
    "\n",
    "print(f\"\\nZero-shot Test Results:\")\n",
    "print(f\"  Accuracy: {zs_acc:.4f}\")\n",
    "print(f\"  Macro-F1: {zs_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "del model_zs\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Complete Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"COMPLETE RESULTS SUMMARY\")\n",
    "print(\"Binary Medical VQA on Chest X-rays (VQA-RAD Dataset)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"  Total filtered samples: {len(samples)}\")\n",
    "print(f\"  Train: {len(train_samples)} | Val: {len(val_samples)} | Test: {len(test_samples)}\")\n",
    "\n",
    "print(f\"\\n{'Model':<30} {'Test Acc':<12} {'Test F1':<12}\")\n",
    "print(\"-\"*55)\n",
    "print(f\"{'ResNet50 + BiLSTM (Baseline)':<30} {cnn_results['test_acc']:.4f}       {cnn_results['test_f1']:.4f}\")\n",
    "print(f\"{'LLaVA-Med Zero-Shot':<30} {zs_acc:.4f}       {zs_f1:.4f}\")\n",
    "print(f\"{'LLaVA-Med + LoRA Fine-tuned':<30} {test_acc:.4f}       {test_f1:.4f}\")\n",
    "print(\"-\"*55)\n",
    "\n",
    "print(f\"\\nKey Findings:\")\n",
    "if test_acc > cnn_results['test_acc']:\n",
    "    print(f\"  - LLaVA-Med (fine-tuned) outperforms CNN baseline by {(test_acc - cnn_results['test_acc'])*100:.1f}% accuracy\")\n",
    "else:\n",
    "    print(f\"  - CNN baseline is competitive, with {(cnn_results['test_acc'] - test_acc)*100:.1f}% higher accuracy\")\n",
    "\n",
    "print(f\"  - Fine-tuning improves LLaVA-Med by {(test_acc - zs_acc)*100:.1f}% over zero-shot\")\n",
    "print(f\"  - LoRA enables efficient adaptation with minimal trainable parameters\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results to JSON\n",
    "results = {\n",
    "    \"dataset\": \"VQA-RAD (Chest X-ray, Binary)\",\n",
    "    \"seed\": SEED,\n",
    "    \"splits\": {\n",
    "        \"train\": len(train_samples),\n",
    "        \"val\": len(val_samples),\n",
    "        \"test\": len(test_samples)\n",
    "    },\n",
    "    \"models\": {\n",
    "        \"cnn_baseline\": cnn_results,\n",
    "        \"llava_med_zero_shot\": {\n",
    "            \"model\": \"LLaVA-Med v1.5 (Zero-Shot)\",\n",
    "            \"test_acc\": zs_acc,\n",
    "            \"test_f1\": zs_f1\n",
    "        },\n",
    "        \"llava_med_finetuned\": {\n",
    "            \"model\": \"LLaVA-Med v1.5 + LoRA\",\n",
    "            \"test_acc\": test_acc,\n",
    "            \"test_f1\": test_f1\n",
    "        }\n",
    "    },\n",
    "    \"training_config\": {\n",
    "        \"lora_r\": 16,\n",
    "        \"lora_alpha\": 32,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"batch_size\": BATCH_SIZE * GRADIENT_ACCUMULATION,\n",
    "        \"epochs_trained\": len(train_losses)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(\"experiment_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"Results saved to experiment_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comparison visualization with all three models\n",
    "models = [\"ResNet50+BiLSTM\\n(CNN)\", \"LLaVA-Med\\n(Zero-Shot)\", \"LLaVA-Med\\n(LoRA Fine-tuned)\"]\n",
    "accuracy = [cnn_results[\"test_acc\"], zs_acc, test_acc]\n",
    "f1_scores = [cnn_results[\"test_f1\"], zs_f1, test_f1]\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bars1 = ax.bar(x - width/2, accuracy, width, label=\"Accuracy\", color=\"steelblue\")\n",
    "bars2 = ax.bar(x + width/2, f1_scores, width, label=\"Macro-F1\", color=\"coral\")\n",
    "\n",
    "ax.set_ylabel(\"Score\", fontsize=12)\n",
    "ax.set_title(\"Complete Model Comparison\\nBinary VQA on Chest X-rays (VQA-RAD)\", fontsize=14)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models, fontsize=11)\n",
    "ax.legend(loc=\"upper right\", fontsize=11)\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "for bar in bars1 + bars2:\n",
    "    height = bar.get_height()\n",
    "    ax.annotate(f'{height:.3f}',\n",
    "                xy=(bar.get_x() + bar.get_width()/2, height),\n",
    "                xytext=(0, 3), textcoords=\"offset points\",\n",
    "                ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.text(2.5, 0.51, 'Random baseline', fontsize=9, color='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"complete_model_comparison.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}